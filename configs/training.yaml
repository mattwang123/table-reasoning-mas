# Training Configuration - OPTIMIZED

# Data paths
data:
  train_file: "data/training/formatted_20251003_135349/train_dataset.jsonl"
  eval_file: "data/training/formatted_20251003_135349/eval_dataset.jsonl"
  training_dir: "data/training"

# Model settings
model:
  name: "Qwen/Qwen2.5-7B-Instruct" #"Qwen/Qwen3-8B" #"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

# LoRA settings - Optimized
lora:
  r: 64
  alpha: 128
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Training settings - OPTIMIZED FOR 2x A100 80GB
training:
  subset_size: null  # Use full dataset (8465 samples)
  eval_subset_size: null  # Larger eval set for better metrics
  
  # OPTIMIZED Core Training Parameters
  batch_size: 12
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8  # ← Increased (was 4)
  eval_accumulation_steps: 4
 
  
  learning_rate: 1e-6
  num_epochs: 1
  warmup_steps: 10
  
  # OPTIMIZED Generation Parameters
  num_generations: 6
  temperature: 0.8
  top_p: 0.9
  beta: 0.02
  max_prompt_length: 4096
  max_completion_length: 1024
  
  # GRPO Optimizations
  shuffle_dataset: true
  mask_truncated_completions: true
  scale_rewards: "group"     # ← TRL default (better than false)
  
  # OPTIMIZED Logging & Checkpointing
  logging_steps: 2
  eval_steps: 200
  save_steps: 200
  
  # Output
  output_dir: "models/verifier_grpo"
  run_name: "table_reasoning_verifier_grpo_optimized"

  # Performance Optimizations
  use_vllm: false

# Wandb configuration - Optimized
wandb:
  enabled: true
  project: "table-reasoning-verifier"
  entity: null
  tags: ["grpo", "table-reasoning", "qwen2.5", "optimized"]
  notes: "Optimized GRPO training for 2x A100 80GB setup"
  save_code: true
  log_model: false
  watch_model: false
  log_freq: 2

# Logging
logging:
  level: "INFO"
  save_logs: true